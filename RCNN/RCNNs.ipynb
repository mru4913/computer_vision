{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Region-based CNNs\n",
    "\n",
    "## Table of Content\n",
    "\n",
    "### 1.0 RCNN\n",
    "### 2.0 Fast RCNN \n",
    "### 3.0 Faster RCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object detection contains two main steps:\n",
    "* Locate bounding boxes containing objects, where each box contain only one object \n",
    "* Classify object inside each bounding boxe and assign label to it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 [RCNN](https://arxiv.org/pdf/1311.2524.pdf) (2013)\n",
    "\n",
    "RCNN was a pioneering approach that applies deep models to object detection, which was introduced by Ross Girshick et al. It is the first CNN that dramatically increases the performance of object detection, with 58.5 mAP on VOC-2007 image dataset.\n",
    "\n",
    "![](./img/rcnn.png)\n",
    "\n",
    "### Process\n",
    "\n",
    "- **1.0** Select multiple high-quality proposed regions from the input image using *SELECTIVE SEARCH* algorithm. These regions are selected with different scales and different shapes. Then label each region class and its bounding box coordinates.\n",
    "- **2.0** A well pretrained CNN (usually VGG or Resnet) is used without the last layer(output layer orclassification layer). It transforms regions input the same input dimensions and performs forward computation to extracgt features from the regions\n",
    "- **3.0** The output features and labels of each proposed region are combined into 1D vector (as an instance) to train multiple support vectore machines for object classification. It means that for each class, it trains a SVM classifier.\n",
    "- **4.0** The output features and labels of each proposed regions are combined intwo 1D vector (as an instance) to train a linear regression model for bounding box prediction.\n",
    "\n",
    "\n",
    "### Key points\n",
    "\n",
    "- It is the first paper that introduce regioned proposed method to detect multiple objects in an image.\n",
    "- It proposes about ~2k regions from each image, which leads to massive forward computation and low efficiency (large needs for memory, slow computation).\n",
    "- It randomly sample some positive examples (regions with object present) and negative examples (regions wihtout object present) during training.\n",
    "- It takes about 13s to predict an image on GPU and 53s on CPU (slow prediction).\n",
    "- It trans models (CNN. SVMs and regressor) separately （multi-stage training also needs more memory). There is no end to end training.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 [Fast RCNN](https://arxiv.org/pdf/1504.08083.pdf) (2015)\n",
    "\n",
    "The main problem with RCNN approach is:\n",
    "- ~2k proposed regions requires massive forward computation. Moreover, these regions highly overlap resulting in a high volume of repetitive computations.\n",
    "\n",
    "In the followup work by Ross Girshick, he proposed a method called Fast R-CNN that significantly speed up object detection. It results 70.0  mAP on VOC-2007+12 dataset.\n",
    "\n",
    "![](./img/frcnn.png)\n",
    "\n",
    "\n",
    "### Process\n",
    "\n",
    "- **1.0** Instead of forward computing ~2k feature maps in RCNN, a Fast RCNN generate only one feature map from the entire image as input. \n",
    "- **2.0** Selective search generates N proposals, thier different shapes indicate regions of interests (RoIs) of different shapes on the feature map. In order to keep the same shapes extracted from these RoIs, an RoI pooling layer is introduced to extracts a fixed-length feature vector, which is finally passed to subsequent FC layers.\n",
    "- **3.0** The shape of the FC layer output is transformed to N x (q+1) for object classification, wehere q is the number of classes and 1 indicates the background.\n",
    "- **4.0** The shape of the FC layer output is transformed to N x 4 for bounding boxes prediction. \n",
    "\n",
    "### Key Points\n",
    "\n",
    "- The architecture is trained end-to-end with a multi-task loss.\n",
    "- Fast RCNN takes the entire image as input.\n",
    "- It needs less because it directly outputs feature maps to FC layers.\n",
    "- During training, it actually randomly sample balanced positve and negative examples. (in paper, it trains 128 boxes each iteration)\n",
    "- Fast RCNN uses L1 loss for bounding box prediction as opposed to L2 loss in R-CNN which is more sensitive to outliers\n",
    "- the present of RoI pooling layer allows us to abandon corp and wrap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Faster RCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faster RCNN was introduced in 2015 but it is still one of the most popular objection detection for most of the researchers. It provides higher accuracy (73.2 mAP on VOC07+12) and faster computation \n",
    "\n",
    "We use a 3 × 3 convolutional layer with a padding of 1 to transform the CNN output and set the number of output channels to c. This way, each element in the feature map the CNN extracts from the image is a new feature with a length of c.\n",
    "2. We use each element in the feature map as a center to generate multiple anchor boxes of different sizes and aspect ratios and then label them.\n",
    "3. We use the features of the elements of length c at the center on the anchor boxes to predict the binary category (object or background) and bounding box for their respective anchor boxes.\n",
    " 10.8. Region-basedCNNs(R-CNNs) 481\n",
    "4. Then, we use non-maximum suppression to remove similar bounding box results that correspond to category predictions of object. Finally, we output the predicted bounding boxes as the proposed regions required by the RoI pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchvision.models' has no attribute 'detection'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f0d004864c08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfasterrcnn_resnet50_fpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torchvision.models' has no attribute 'detection'"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pytorch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e0abc4405399>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pytorch' is not defined"
     ]
    }
   ],
   "source": [
    "print(pytorch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
